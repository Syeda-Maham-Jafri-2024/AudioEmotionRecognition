{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":8129178,"sourceType":"datasetVersion","datasetId":4804674},{"sourceId":8129230,"sourceType":"datasetVersion","datasetId":4804713},{"sourceId":8238522,"sourceType":"datasetVersion","datasetId":4886898}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nimport time\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/librosaextractedtess/ExtractedFeaturesForTessDatasetLibrosaCleanedNa.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Standardize features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN input\nX_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\nX_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n\n\n# Initializing models with improved parameters\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, C= 0.001),\n    'Naive Bayes': GaussianNB(var_smoothing=1e-11),\n    'Decision Tree': DecisionTreeClassifier(max_depth= 7, min_samples_split=5),\n    'Random Forest': RandomForestClassifier(n_estimators=1000, max_depth=10, min_samples_split=5),\n    'KNN': KNeighborsClassifier(n_neighbors=15),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth= 10, subsample=0.8),\n    'XGBoost': xgb.XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth= 10, subsample=0.8),\n    'LightGBM': lgb.LGBMClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8),\n   # Modified architectures for neural network models\n  'Feedforward Neural Network': Sequential([\n    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(len(label_encoder.classes_), activation='softmax')\n   ]),\n# Convolutional Neural Network\n  'Convolutional Neural Network': Sequential([\n    Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(len(label_encoder.classes_), activation='softmax')\n  ]),\n# Deep Belief Network\n  'Deep Belief Network': Sequential([\n    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(len(label_encoder.classes_), activation='softmax')\n   ]),\n    'Voting Classifier': VotingClassifier(estimators=[\n        ('Logistic Regression', LogisticRegression(max_iter=1000, C= 0.001)),\n        ('Naive Bayes', GaussianNB(var_smoothing=1e-11)),\n        ('Decision Tree', DecisionTreeClassifier(max_depth=7, min_samples_split=5)),\n        ('Random Forest', RandomForestClassifier(n_estimators=1000, max_depth=10, min_samples_split=5)),\n        ('KNN', KNeighborsClassifier(n_neighbors=15)),\n        ('Gradient Boosting', GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth=10, subsample=0.8)),\n        ('XGBoost', xgb.XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8)),\n        ('LightGBM', lgb.LGBMClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8)),\n    ]),\n'MLP Classifier': MLPClassifier(\n    hidden_layer_sizes=(256, 128, 64),  # Increased hidden layer sizes\n    max_iter=1000,  # Increased maximum number of iterations\n    activation='relu',  # ReLU activation function\n    solver='adam',  # Adam optimizer\n    random_state=42\n)\n}\n\n# Training and evaluating each model\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_cnn, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    else:\n        model.fit(X_train, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T14:14:56.939310Z","iopub.execute_input":"2024-04-23T14:14:56.939713Z","iopub.status.idle":"2024-04-23T19:34:13.742223Z","shell.execute_reply.started":"2024-04-23T14:14:56.939681Z","shell.execute_reply":"2024-04-23T19:34:13.740576Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-23 14:15:02.839572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-23 14:15:02.839707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-23 14:15:03.025475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression: Accuracy = 0.7875, Precision = 0.7901, Recall = 0.7875, F1 Score = 0.7852, Training Time = 10.7022 s, Prediction Time = 0.0434 s\nNaive Bayes: Accuracy = 0.6679, Precision = 0.7409, Recall = 0.6679, F1 Score = 0.6758, Training Time = 0.1233 s, Prediction Time = 0.0537 s\nDecision Tree: Accuracy = 0.8589, Precision = 0.8699, Recall = 0.8589, F1 Score = 0.8577, Training Time = 2.1491 s, Prediction Time = 0.0243 s\nRandom Forest: Accuracy = 0.9554, Precision = 0.9568, Recall = 0.9554, F1 Score = 0.9554, Training Time = 52.9638 s, Prediction Time = 0.1749 s\nKNN: Accuracy = 0.4232, Precision = 0.5381, Recall = 0.4232, F1 Score = 0.4158, Training Time = 0.0372 s, Prediction Time = 0.2146 s\nGradient Boosting: Accuracy = 0.9679, Precision = 0.9682, Recall = 0.9679, F1 Score = 0.9679, Training Time = 8341.0641 s, Prediction Time = 0.1760 s\nXGBoost: Accuracy = 0.9768, Precision = 0.9773, Recall = 0.9768, F1 Score = 0.9769, Training Time = 553.5942 s, Prediction Time = 0.1992 s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 324900\n[LightGBM] [Info] Number of data points in the train set: 2240, number of used features: 1304\n[LightGBM] [Info] Start training from score -1.964838\n[LightGBM] [Info] Start training from score -1.927334\n[LightGBM] [Info] Start training from score -1.974438\n[LightGBM] [Info] Start training from score -1.955329\n[LightGBM] [Info] Start training from score -1.912113\n[LightGBM] [Info] Start training from score -1.974438\n[LightGBM] [Info] Start training from score -1.915138\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nLightGBM: Accuracy = 0.9839, Precision = 0.9841, Recall = 0.9839, F1 Score = 0.9839, Training Time = 388.6260 s, Prediction Time = 0.1626 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nFeedforward Neural Network: Accuracy = 0.7304, Precision = 0.7473, Recall = 0.7304, F1 Score = 0.7267, Training Time = 6.7968 s, Prediction Time = 0.3861 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step\nConvolutional Neural Network: Accuracy = 0.8661, Precision = 0.8728, Recall = 0.8661, F1 Score = 0.8679, Training Time = 297.4154 s, Prediction Time = 1.3316 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nDeep Belief Network: Accuracy = 0.7625, Precision = 0.7647, Recall = 0.7625, F1 Score = 0.7587, Training Time = 5.7956 s, Prediction Time = 0.2097 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074873 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 324900\n[LightGBM] [Info] Number of data points in the train set: 2240, number of used features: 1304\n[LightGBM] [Info] Start training from score -1.964838\n[LightGBM] [Info] Start training from score -1.927334\n[LightGBM] [Info] Start training from score -1.974438\n[LightGBM] [Info] Start training from score -1.955329\n[LightGBM] [Info] Start training from score -1.912113\n[LightGBM] [Info] Start training from score -1.974438\n[LightGBM] [Info] Start training from score -1.915138\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nVoting Classifier: Accuracy = 0.9821, Precision = 0.9823, Recall = 0.9821, F1 Score = 0.9821, Training Time = 9412.4224 s, Prediction Time = 1.2157 s\nMLP Classifier: Accuracy = 0.7554, Precision = 0.7596, Recall = 0.7554, F1 Score = 0.7522, Training Time = 58.8225 s, Prediction Time = 0.0538 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"HYPERPARAMETERS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport xgboost as xgb\nimport lightgbm as lgb\nimport time\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/librosaextractedtess/ExtractedFeaturesForTessDatasetLibrosaCleanedNa.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Define hyperparameters for each model\nparam_grid = {\n    'Logistic Regression': {'C': [0.1, 1, 10]},\n    'Decision Tree': {'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]},\n    'Random Forest': {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]},\n    'KNN': {'n_neighbors': [3, 5, 7]},\n    'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]},\n    'XGBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]},\n    'LightGBM': {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n}\n\n# Initialize models with default parameters\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'KNN': KNeighborsClassifier(),\n    'Gradient Boosting': GradientBoostingClassifier(),\n    'XGBoost': xgb.XGBClassifier(),\n    'LightGBM': lgb.LGBMClassifier()\n}\n\n# Initialize GridSearchCV objects for hyperparameter tuning\ngrid_searches = {}\nfor name, model in models.items():\n    param_grid_model = param_grid.get(name, {})  # Get hyperparameters for the current model\n    grid_search = GridSearchCV(estimator=model, param_grid=param_grid_model, scoring='accuracy', cv=5, n_jobs=-1)\n    grid_searches[name] = grid_search\n\n# Training and evaluating each model with hyperparameter tuning\nfor name, grid_search in grid_searches.items():\n    start_time = time.time()\n    \n    grid_search.fit(X_train_scaled, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    best_model = grid_search.best_estimator_\n    y_pred = best_model.predict(X_test_scaled)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Best Params = {grid_search.best_params_}, Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T15:00:31.847732Z","iopub.execute_input":"2024-04-24T15:00:31.848132Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logistic Regression: Best Params = {'C': 10}, Accuracy = 0.9554, Precision = 0.9562, Recall = 0.9554, F1 Score = 0.9555, Training Time = 59.6209 s\nDecision Tree: Best Params = {'max_depth': 10, 'min_samples_split': 2}, Accuracy = 0.8893, Precision = 0.8906, Recall = 0.8893, F1 Score = 0.8892, Training Time = 39.4901 s\nRandom Forest: Best Params = {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}, Accuracy = 0.9554, Precision = 0.9569, Recall = 0.9554, F1 Score = 0.9555, Training Time = 450.8667 s\nKNN: Best Params = {'n_neighbors': 3}, Accuracy = 0.7482, Precision = 0.7707, Recall = 0.7482, F1 Score = 0.7451, Training Time = 1.2483 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Models of LR","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nimport xgboost as xgb\nimport lightgbm as lgb\nimport time\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/librosaextractedtess/ExtractedFeaturesForTessDatasetLibrosaCleanedNa.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN input\nX_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\nX_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n\n#solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}\n# Initializing models\nmodels = {\n    'Logistic Regression0': LogisticRegression(max_iter=1000, penalty=\"l2\"),\n    'Logistic Regression1': LogisticRegression(max_iter=1000, penalty=\"none\"),\n    'Logistic Regression2': LogisticRegression(max_iter=1000, solver=\"liblinear\", penalty=\"l1\"),\n    'Logistic Regression3': LogisticRegression(max_iter=1000, solver=\"liblinear\", penalty=\"l2\"),\n    'Logistic Regression4': LogisticRegression(max_iter=1000, solver=\"newton-cg\", penalty=\"l2\"),\n    'Logistic Regression5': LogisticRegression(max_iter=1000, solver=\"newton-cg\", penalty=\"none\"),\n    'Logistic Regression6': LogisticRegression(max_iter=1000, solver=\"newton-cholesky\", penalty=\"l2\"),\n    'Logistic Regression7': LogisticRegression(max_iter=1000, solver=\"newton-cholesky\", penalty=\"none\"),\n    'Logistic Regression8': LogisticRegression(max_iter=1000, solver=\"sag\", penalty=\"l2\"),\n    'Logistic Regression9': LogisticRegression(max_iter=1000, solver=\"sag\", penalty=\"none\"),\n    'Logistic Regression10': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"l2\"),\n    'Logistic Regression11': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5),\n    'Logistic Regression12': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"l1\"),\n    'Logistic Regression13': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"none\"),\n    'Logistic Regression14': LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n    'Logistic Regression15': LogisticRegression(max_iter=1000, C=10),\n    'Logistic Regression16': LogisticRegression(max_iter=1000, C=0.001),\n    'Logistic Regression17': LogisticRegression(max_iter=1000, C=0.1),\n}\n\n\n\n# Training and evaluating each model\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_cnn, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    else:\n        model.fit(X_train, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T18:30:31.490356Z","iopub.execute_input":"2024-04-25T18:30:31.490748Z","iopub.status.idle":"2024-04-25T19:11:34.510194Z","shell.execute_reply.started":"2024-04-25T18:30:31.490715Z","shell.execute_reply":"2024-04-25T19:11:34.509067Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-25 18:30:35.936264: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-25 18:30:35.936359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-25 18:30:36.062757: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression0: Accuracy = 0.8071, Precision = 0.8114, Recall = 0.8071, F1 Score = 0.8065, Training Time = 10.1072 s, Prediction Time = 0.0421 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression1: Accuracy = 0.8107, Precision = 0.8131, Recall = 0.8107, F1 Score = 0.8089, Training Time = 8.9106 s, Prediction Time = 0.0423 s\nLogistic Regression2: Accuracy = 0.8804, Precision = 0.8823, Recall = 0.8804, F1 Score = 0.8785, Training Time = 138.8424 s, Prediction Time = 0.0252 s\nLogistic Regression3: Accuracy = 0.8571, Precision = 0.8619, Recall = 0.8571, F1 Score = 0.8547, Training Time = 116.4706 s, Prediction Time = 0.0343 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/optimize.py:210: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression4: Accuracy = 0.8375, Precision = 0.8433, Recall = 0.8375, F1 Score = 0.8355, Training Time = 799.3787 s, Prediction Time = 0.0421 s\nLogistic Regression5: Accuracy = 0.8107, Precision = 0.8175, Recall = 0.8107, F1 Score = 0.8080, Training Time = 39.4196 s, Prediction Time = 0.0418 s\nLogistic Regression6: Accuracy = 0.8482, Precision = 0.8514, Recall = 0.8482, F1 Score = 0.8452, Training Time = 28.3227 s, Prediction Time = 0.0707 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:195: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nMatrix is singular.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:195: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression7: Accuracy = 0.8071, Precision = 0.8153, Recall = 0.8071, F1 Score = 0.8075, Training Time = 11.0187 s, Prediction Time = 0.0432 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression8: Accuracy = 0.7411, Precision = 0.7456, Recall = 0.7411, F1 Score = 0.7385, Training Time = 123.2374 s, Prediction Time = 0.0248 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression9: Accuracy = 0.7411, Precision = 0.7456, Recall = 0.7411, F1 Score = 0.7385, Training Time = 123.2694 s, Prediction Time = 0.0312 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression10: Accuracy = 0.7018, Precision = 0.7052, Recall = 0.7018, F1 Score = 0.6974, Training Time = 146.6237 s, Prediction Time = 0.0255 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression11: Accuracy = 0.7018, Precision = 0.7052, Recall = 0.7018, F1 Score = 0.6974, Training Time = 357.1472 s, Prediction Time = 0.0249 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression12: Accuracy = 0.7018, Precision = 0.7048, Recall = 0.7018, F1 Score = 0.6975, Training Time = 359.8141 s, Prediction Time = 0.0296 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression13: Accuracy = 0.7018, Precision = 0.7052, Recall = 0.7018, F1 Score = 0.6974, Training Time = 146.1248 s, Prediction Time = 0.0244 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression14: Accuracy = 0.7911, Precision = 0.7953, Recall = 0.7911, F1 Score = 0.7889, Training Time = 8.9063 s, Prediction Time = 0.0419 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression15: Accuracy = 0.7946, Precision = 0.7975, Recall = 0.7946, F1 Score = 0.7917, Training Time = 8.9724 s, Prediction Time = 0.0430 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression16: Accuracy = 0.7875, Precision = 0.7901, Recall = 0.7875, F1 Score = 0.7852, Training Time = 9.8156 s, Prediction Time = 0.0413 s\nLogistic Regression17: Accuracy = 0.7893, Precision = 0.7942, Recall = 0.7893, F1 Score = 0.7870, Training Time = 8.9973 s, Prediction Time = 0.0417 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"wav2vec feature extractiom","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nimport torchaudio\nfrom torchaudio.transforms import Resample\nimport numpy as np\nimport time\n\nclass EmotionModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.wav2vec2 = Wav2Vec2Model(config)\n\n    def forward(self, input_values):\n        outputs = self.wav2vec2(input_values)\n        hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0)\n        return hidden_states\n\ndef process_func(audio_dir):\n    device = 'cpu'\n    model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n    processor = Wav2Vec2Processor.from_pretrained(model_name)\n    model = Wav2Vec2Model.from_pretrained(model_name)\n    model.to(device)\n\n    features_list = []\n    paths_list = []\n\n    resampler = Resample(orig_freq=24414, new_freq=16000)\n\n    start_time = time.time()\n\n    for root, dirs, files in os.walk(audio_dir):\n        for file in files:\n            audio_path = os.path.join(root, file)\n            array, fs = torchaudio.load(audio_path)\n            array_resampled = resampler(array)\n            input_values = processor(array_resampled.squeeze(), sampling_rate=16000, return_tensors=\"pt\")\n            input_values = input_values.input_values.to(device)\n            with torch.no_grad():\n                hidden_states = model(input_values)\n            features_list.append(hidden_states.last_hidden_state.squeeze().mean(axis=0).cpu().numpy().tolist())\n            paths_list.append(audio_path)\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(\"Time taken to extract features:\", elapsed_time, \"seconds\")\n\n    return features_list, paths_list\n\n# Example usage:\naudio_dir = '/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data'\nfeatures_list, paths_list = process_func(audio_dir)\n\n# Convert lists to DataFrame\ndata = {'Path': paths_list}\nfor i in range(len(features_list[0])):\n    data[f'Feature_{i}'] = [feature[i] for feature in features_list]\n\ndf = pd.DataFrame(data)\n\n# Save DataFrame to CSV\ncsv_file = '/kaggle/working/featuresforTessWav2vec.csv'\ndf.to_csv(csv_file, index=False)\nprint(\"Features saved to:\", csv_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:11:04.137549Z","iopub.execute_input":"2024-04-26T15:11:04.138063Z","iopub.status.idle":"2024-04-26T15:53:18.418682Z","shell.execute_reply.started":"2024-04-26T15:11:04.138031Z","shell.execute_reply":"2024-04-26T15:53:18.417249Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of Wav2Vec2Model were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Time taken to extract features: 2517.401907682419 seconds\nFeatures saved to: /kaggle/working/featuresforTessWav2vec.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv')\ndata.head()\n#31 mis","metadata":{"execution":{"iopub.status.busy":"2024-04-26T16:59:29.494906Z","iopub.execute_input":"2024-04-26T16:59:29.495767Z","iopub.status.idle":"2024-04-26T16:59:31.121001Z","shell.execute_reply.started":"2024-04-26T16:59:29.495721Z","shell.execute_reply":"2024-04-26T16:59:31.119506Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                Path  Feature_0  Feature_1  \\\n0  /kaggle/input/toronto-emotional-speech-set-tes...  -0.007627   0.006393   \n1  /kaggle/input/toronto-emotional-speech-set-tes...  -0.007651   0.006757   \n2  /kaggle/input/toronto-emotional-speech-set-tes...  -0.007654   0.007805   \n3  /kaggle/input/toronto-emotional-speech-set-tes...  -0.007416   0.007406   \n4  /kaggle/input/toronto-emotional-speech-set-tes...  -0.007399   0.006563   \n\n   Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  Feature_7  \\\n0  -0.009016  -0.006457  -0.005413  -0.007740  -0.079955   0.005843   \n1  -0.008650  -0.004695  -0.005046  -0.008044  -0.078741   0.006259   \n2  -0.008533  -0.005969  -0.005547  -0.007536   0.011286   0.006206   \n3  -0.007871  -0.006976  -0.005285  -0.008491  -0.080460   0.005944   \n4  -0.007964  -0.005912  -0.005460  -0.008129  -0.074036   0.006423   \n\n   Feature_8  ...  Feature_1015  Feature_1016  Feature_1017  Feature_1018  \\\n0   0.079756  ...      0.010743      0.009884      0.008419      0.005200   \n1   0.028489  ...      0.010798      0.045597      0.008512      0.005134   \n2   0.100660  ...      0.010882      0.014696      0.008473      0.043847   \n3   0.120987  ...      0.011316     -0.004393      0.008385      0.005210   \n4   0.064441  ...      0.010993     -0.006457      0.008377     -0.001801   \n\n   Feature_1019  Feature_1020  Feature_1021  Feature_1022  Feature_1023  \\\n0     -0.031274      0.333547      0.006994      0.009229      0.003448   \n1     -0.082161      0.314790      0.007343      0.009305      0.004567   \n2     -0.053118      0.300673      0.007296      0.009384      0.004460   \n3      0.001423      0.202869      0.007186      0.009218      0.005208   \n4     -0.090610      0.247391      0.007254      0.009252      0.005467   \n\n   Emotions  \n0      fear  \n1      fear  \n2      fear  \n3      fear  \n4      fear  \n\n[5 rows x 1026 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Path</th>\n      <th>Feature_0</th>\n      <th>Feature_1</th>\n      <th>Feature_2</th>\n      <th>Feature_3</th>\n      <th>Feature_4</th>\n      <th>Feature_5</th>\n      <th>Feature_6</th>\n      <th>Feature_7</th>\n      <th>Feature_8</th>\n      <th>...</th>\n      <th>Feature_1015</th>\n      <th>Feature_1016</th>\n      <th>Feature_1017</th>\n      <th>Feature_1018</th>\n      <th>Feature_1019</th>\n      <th>Feature_1020</th>\n      <th>Feature_1021</th>\n      <th>Feature_1022</th>\n      <th>Feature_1023</th>\n      <th>Emotions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n      <td>-0.007627</td>\n      <td>0.006393</td>\n      <td>-0.009016</td>\n      <td>-0.006457</td>\n      <td>-0.005413</td>\n      <td>-0.007740</td>\n      <td>-0.079955</td>\n      <td>0.005843</td>\n      <td>0.079756</td>\n      <td>...</td>\n      <td>0.010743</td>\n      <td>0.009884</td>\n      <td>0.008419</td>\n      <td>0.005200</td>\n      <td>-0.031274</td>\n      <td>0.333547</td>\n      <td>0.006994</td>\n      <td>0.009229</td>\n      <td>0.003448</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n      <td>-0.007651</td>\n      <td>0.006757</td>\n      <td>-0.008650</td>\n      <td>-0.004695</td>\n      <td>-0.005046</td>\n      <td>-0.008044</td>\n      <td>-0.078741</td>\n      <td>0.006259</td>\n      <td>0.028489</td>\n      <td>...</td>\n      <td>0.010798</td>\n      <td>0.045597</td>\n      <td>0.008512</td>\n      <td>0.005134</td>\n      <td>-0.082161</td>\n      <td>0.314790</td>\n      <td>0.007343</td>\n      <td>0.009305</td>\n      <td>0.004567</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n      <td>-0.007654</td>\n      <td>0.007805</td>\n      <td>-0.008533</td>\n      <td>-0.005969</td>\n      <td>-0.005547</td>\n      <td>-0.007536</td>\n      <td>0.011286</td>\n      <td>0.006206</td>\n      <td>0.100660</td>\n      <td>...</td>\n      <td>0.010882</td>\n      <td>0.014696</td>\n      <td>0.008473</td>\n      <td>0.043847</td>\n      <td>-0.053118</td>\n      <td>0.300673</td>\n      <td>0.007296</td>\n      <td>0.009384</td>\n      <td>0.004460</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n      <td>-0.007416</td>\n      <td>0.007406</td>\n      <td>-0.007871</td>\n      <td>-0.006976</td>\n      <td>-0.005285</td>\n      <td>-0.008491</td>\n      <td>-0.080460</td>\n      <td>0.005944</td>\n      <td>0.120987</td>\n      <td>...</td>\n      <td>0.011316</td>\n      <td>-0.004393</td>\n      <td>0.008385</td>\n      <td>0.005210</td>\n      <td>0.001423</td>\n      <td>0.202869</td>\n      <td>0.007186</td>\n      <td>0.009218</td>\n      <td>0.005208</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/toronto-emotional-speech-set-tes...</td>\n      <td>-0.007399</td>\n      <td>0.006563</td>\n      <td>-0.007964</td>\n      <td>-0.005912</td>\n      <td>-0.005460</td>\n      <td>-0.008129</td>\n      <td>-0.074036</td>\n      <td>0.006423</td>\n      <td>0.064441</td>\n      <td>...</td>\n      <td>0.010993</td>\n      <td>-0.006457</td>\n      <td>0.008377</td>\n      <td>-0.001801</td>\n      <td>-0.090610</td>\n      <td>0.247391</td>\n      <td>0.007254</td>\n      <td>0.009252</td>\n      <td>0.005467</td>\n      <td>fear</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1026 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file into a DataFrame\ncsv_file = '/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv\ndf = pd.read_csv(csv_file)\n\n# Get the number of rows and columns\nnum_rows, num_cols = df.shape\n\n# Print the number of rows and columns\nprint(\"Number of rows:\", num_rows)\nprint(\"Number of columns:\", num_cols)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:56:48.663221Z","iopub.execute_input":"2024-04-26T15:56:48.663698Z","iopub.status.idle":"2024-04-26T15:56:49.755705Z","shell.execute_reply.started":"2024-04-26T15:56:48.663668Z","shell.execute_reply":"2024-04-26T15:56:49.754303Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Number of rows: 2800\nNumber of columns: 1025\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the first CSV file\ncsv1 = pd.read_csv(\"/kaggle/input/librosaextractedtess/ExtractedFeaturesForTessDatasetLibrosaCleanedNa.csv\")\n\n# Read the second CSV file\ncsv2 = pd.read_csv(\"/kaggle/input/wav2vecextractedtess/featuresforTessWav2vec.csv\")\n\n# Copy the 'Emotions' column from csv1 to csv2\ncsv2['Emotions'] = csv1['Emotions']\n\n# Save the modified csv2 to a new CSV file\ncsv2.to_csv(\"/kaggle/working/ExtractedFeaturesForTessDatasetWav2Vec.csv\", index=False)\n\nprint(\"Emotions column copied from csv1 to csv2 and saved to output.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T16:14:59.975047Z","iopub.execute_input":"2024-04-26T16:14:59.976517Z","iopub.status.idle":"2024-04-26T16:15:09.827566Z","shell.execute_reply.started":"2024-04-26T16:14:59.976465Z","shell.execute_reply":"2024-04-26T16:15:09.826291Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Emotions column copied from csv1 to csv2 and saved to output.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file into a DataFrame\ncsv_file_path = \"/kaggle/working/ExtractedFeaturesForTessDatasetWav2Vec.csv\"\ndata = pd.read_csv(csv_file_path)\n\n# Print a particular column (e.g., \"emotion\")\nprint(data[\"Emotions\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T16:15:16.244521Z","iopub.execute_input":"2024-04-26T16:15:16.245467Z","iopub.status.idle":"2024-04-26T16:15:17.234536Z","shell.execute_reply.started":"2024-04-26T16:15:16.245423Z","shell.execute_reply":"2024-04-26T16:15:17.233168Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"0           fear\n1           fear\n2           fear\n3           fear\n4           fear\n          ...   \n2795    surprise\n2796    surprise\n2797    surprise\n2798    surprise\n2799    surprise\nName: Emotions, Length: 2800, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file\ncsv_file_path = \"/kaggle/working/features.csv\"\ndata = pd.read_csv(csv_file_path)\n\n# Split the values in the \"Features\" column based on the delimiter \" \"\ndata['Features'] = data['Features'].str.split(\" \")\n\n# Expand the list of values into separate columns\ndata_expanded = data['Features'].apply(pd.Series)\n\n# Rename the columns if needed\n# data_expanded = data_expanded.rename(columns=lambda x: f\"feature_{x}\")\n\n# Save the modified DataFrame to a new CSV file\nnew_csv_file_path = \"Wav2vectessfeatures.csv\"\ndata_expanded.to_csv(new_csv_file_path, index=False)\n\n# Print the first few rows of the modified DataFrame\nprint(data_expanded.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:04:38.018751Z","iopub.execute_input":"2024-04-26T15:04:38.019286Z","iopub.status.idle":"2024-04-26T15:04:38.626015Z","shell.execute_reply.started":"2024-04-26T15:04:38.019251Z","shell.execute_reply":"2024-04-26T15:04:38.624632Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"             0  1           2            3            4    5           6   \\\n0  [-0.00762739     0.00639291  -0.00901586          ...       0.00699424   \n1  [-0.00765135     0.00675729  -0.00864979          ...        0.0073433   \n2  [-0.00765371     0.00780493  -0.00853262          ...       0.00729563   \n3  [-0.00741571     0.00740618  -0.00787141          ...       0.00718567   \n4   [-0.0073986                  0.00656308  -0.00796391  ...               \n\n           7             8             9            10           11   12   13  \\\n0               0.0092293\\n                0.00344799]          NaN  NaN  NaN   \n1                            0.00930463\\n               0.00456725]  NaN  NaN   \n2              0.00938383\\n                0.00445961]          NaN  NaN  NaN   \n3              0.00921761\\n                0.00520833]          NaN  NaN  NaN   \n4  0.00725434                0.00925199\\n                 0.0054665    ]  NaN   \n\n    14   15  \n0  NaN  NaN  \n1  NaN  NaN  \n2  NaN  NaN  \n3  NaN  NaN  \n4  NaN  NaN  \n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/working/Wav2vectessfeatures.csv')\ndata.head()\n#31 mis","metadata":{"execution":{"iopub.status.busy":"2024-04-26T15:05:21.911436Z","iopub.execute_input":"2024-04-26T15:05:21.911884Z","iopub.status.idle":"2024-04-26T15:05:21.953501Z","shell.execute_reply.started":"2024-04-26T15:05:21.911854Z","shell.execute_reply":"2024-04-26T15:05:21.952305Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"              0   1         2         3            4    5           6  \\\n0  [-0.00762739 NaN  0.006393 -0.009016          ...  NaN  0.00699424   \n1  [-0.00765135 NaN  0.006757 -0.008650          ...  NaN   0.0073433   \n2  [-0.00765371 NaN  0.007805 -0.008533          ...  NaN  0.00729563   \n3  [-0.00741571 NaN  0.007406 -0.007871          ...  NaN  0.00718567   \n4   [-0.0073986 NaN       NaN  0.006563  -0.00796391  ...         NaN   \n\n            7             8         9           10           11   12   13  \\\n0         NaN   0.0092293\\n       NaN  0.00344799]          NaN  NaN  NaN   \n1         NaN           NaN  0.009305          NaN  0.00456725]  NaN  NaN   \n2         NaN  0.00938383\\n       NaN  0.00445961]          NaN  NaN  NaN   \n3         NaN  0.00921761\\n       NaN  0.00520833]          NaN  NaN  NaN   \n4  0.00725434           NaN  0.009252          NaN    0.0054665    ]  NaN   \n\n    14   15  \n0  NaN  NaN  \n1  NaN  NaN  \n2  NaN  NaN  \n3  NaN  NaN  \n4  NaN  NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.00762739</td>\n      <td>NaN</td>\n      <td>0.006393</td>\n      <td>-0.009016</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.00699424</td>\n      <td>NaN</td>\n      <td>0.0092293\\n</td>\n      <td>NaN</td>\n      <td>0.00344799]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-0.00765135</td>\n      <td>NaN</td>\n      <td>0.006757</td>\n      <td>-0.008650</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.0073433</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.009305</td>\n      <td>NaN</td>\n      <td>0.00456725]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-0.00765371</td>\n      <td>NaN</td>\n      <td>0.007805</td>\n      <td>-0.008533</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.00729563</td>\n      <td>NaN</td>\n      <td>0.00938383\\n</td>\n      <td>NaN</td>\n      <td>0.00445961]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[-0.00741571</td>\n      <td>NaN</td>\n      <td>0.007406</td>\n      <td>-0.007871</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.00718567</td>\n      <td>NaN</td>\n      <td>0.00921761\\n</td>\n      <td>NaN</td>\n      <td>0.00520833]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[-0.0073986</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.006563</td>\n      <td>-0.00796391</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.00725434</td>\n      <td>NaN</td>\n      <td>0.009252</td>\n      <td>NaN</td>\n      <td>0.0054665</td>\n      <td>]</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"SVM C","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.svm import SVC\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU, LSTM, Embedding, Bidirectional, Dropout\nimport time\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport warnings\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions','Path'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initializing models\nmodels = {\n    'Support Vector Machine': SVC(),\n    'Recurrent Neural Network': Sequential([\n        Embedding(input_dim=X_train_scaled.shape[1], output_dim=64),\n        LSTM(64),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Gated Recurrent Unit': Sequential([\n        Embedding(input_dim=X_train_scaled.shape[1], output_dim=64),\n        GRU(64),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Long Short-Term Memory': Sequential([\n        Embedding(input_dim=X_train_scaled.shape[1], output_dim=64),\n        Bidirectional(LSTM(64)),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ])\n}\n\n# Training and evaluating each model\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Support Vector Machine':\n        model.fit(X_train_scaled, y_train_encoded)\n    else:\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name == 'Support Vector Machine':\n        y_pred = model.predict(X_test_scaled)\n    else:\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T06:44:19.514760Z","iopub.execute_input":"2024-05-01T06:44:19.515252Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Support Vector Machine: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.9212 s, Prediction Time = 0.4704 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step\nRecurrent Neural Network: Accuracy = 0.3179, Precision = 0.2945, Recall = 0.3179, F1 Score = 0.2771, Training Time = 381.5648 s, Prediction Time = 2.4898 s\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nimport xgboost as xgb\nimport lightgbm as lgb\nimport time\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions','Path'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN input\nX_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\nX_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n\n#solver{‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}\n# Initializing models\nmodels = {\n    'Logistic Regression0': LogisticRegression(max_iter=1000, penalty=\"l2\"),\n    'Logistic Regression1': LogisticRegression(max_iter=1000, penalty=\"none\"),\n    'Logistic Regression2': LogisticRegression(max_iter=1000, solver=\"liblinear\", penalty=\"l1\"),\n    'Logistic Regression3': LogisticRegression(max_iter=1000, solver=\"liblinear\", penalty=\"l2\"),\n    'Logistic Regression4': LogisticRegression(max_iter=1000, solver=\"newton-cg\", penalty=\"l2\"),\n    'Logistic Regression5': LogisticRegression(max_iter=1000, solver=\"newton-cg\", penalty=\"none\"),\n    'Logistic Regression6': LogisticRegression(max_iter=1000, solver=\"newton-cholesky\", penalty=\"l2\"),\n    'Logistic Regression7': LogisticRegression(max_iter=1000, solver=\"newton-cholesky\", penalty=\"none\"),\n    'Logistic Regression8': LogisticRegression(max_iter=1000, solver=\"sag\", penalty=\"l2\"),\n    'Logistic Regression9': LogisticRegression(max_iter=1000, solver=\"sag\", penalty=\"none\"),\n    'Logistic Regression10': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"l2\"),\n    'Logistic Regression11': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5),\n    'Logistic Regression12': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"l1\"),\n    'Logistic Regression13': LogisticRegression(max_iter=1000, solver=\"saga\", penalty=\"none\"),\n    'Logistic Regression14': LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n    'Logistic Regression15': LogisticRegression(max_iter=1000, C=10),\n    'Logistic Regression16': LogisticRegression(max_iter=1000, C=0.001),\n    'Logistic Regression17': LogisticRegression(max_iter=1000, C=0.1),\n}\n\n\n\n# Training and evaluating each model\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_cnn, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n    else:\n        model.fit(X_train, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T09:37:52.751563Z","iopub.execute_input":"2024-05-01T09:37:52.751936Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Logistic Regression0: Accuracy = 0.9982, Precision = 0.9982, Recall = 0.9982, F1 Score = 0.9982, Training Time = 1.6470 s, Prediction Time = 0.0657 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression1: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.5571 s, Prediction Time = 0.0304 s\nLogistic Regression2: Accuracy = 0.9964, Precision = 0.9965, Recall = 0.9964, F1 Score = 0.9964, Training Time = 1.9662 s, Prediction Time = 0.0186 s\nLogistic Regression3: Accuracy = 0.9964, Precision = 0.9965, Recall = 0.9964, F1 Score = 0.9964, Training Time = 4.1868 s, Prediction Time = 0.0191 s\nLogistic Regression4: Accuracy = 0.9982, Precision = 0.9982, Recall = 0.9982, F1 Score = 0.9982, Training Time = 0.8492 s, Prediction Time = 0.0319 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression5: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 1.0405 s, Prediction Time = 0.0327 s\nLogistic Regression6: Accuracy = 0.9982, Precision = 0.9982, Recall = 0.9982, F1 Score = 0.9982, Training Time = 9.3366 s, Prediction Time = 0.0304 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_glm/_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\nFurther options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\nThe original Linear Algebra message was:\nIll-conditioned matrix (rcond=1.28353e-21): result may not be accurate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression7: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 2.2471 s, Prediction Time = 0.0322 s\nLogistic Regression8: Accuracy = 0.9982, Precision = 0.9982, Recall = 0.9982, F1 Score = 0.9982, Training Time = 2.9399 s, Prediction Time = 0.0211 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nimport time\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions','Path'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Standardize features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN input\nX_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\nX_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n\n\n# Initializing models with improved parameters\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, C= 0.001),\n    'Naive Bayes': GaussianNB(var_smoothing=1e-11),\n    'Decision Tree': DecisionTreeClassifier(max_depth= 7, min_samples_split=5),\n    'Random Forest': RandomForestClassifier(n_estimators=1000, max_depth=10, min_samples_split=5),\n    'KNN': KNeighborsClassifier(n_neighbors=15),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth= 10, subsample=0.8),\n    'XGBoost': xgb.XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth= 10, subsample=0.8),\n    'LightGBM': lgb.LGBMClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8),\n   # Modified architectures for neural network models\n  'Feedforward Neural Network': Sequential([\n    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(len(label_encoder.classes_), activation='softmax')\n   ]),\n# Convolutional Neural Network\n  'Convolutional Neural Network': Sequential([\n    Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),\n    MaxPooling1D(pool_size=2),\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(len(label_encoder.classes_), activation='softmax')\n  ]),\n# Deep Belief Network\n  'Deep Belief Network': Sequential([\n    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dense(len(label_encoder.classes_), activation='softmax')\n   ]),\n    'Voting Classifier': VotingClassifier(estimators=[\n        ('Logistic Regression', LogisticRegression(max_iter=1000, C= 0.001)),\n        ('Naive Bayes', GaussianNB(var_smoothing=1e-11)),\n        ('Decision Tree', DecisionTreeClassifier(max_depth=7, min_samples_split=5)),\n        ('Random Forest', RandomForestClassifier(n_estimators=1000, max_depth=10, min_samples_split=5)),\n        ('KNN', KNeighborsClassifier(n_neighbors=15)),\n        ('Gradient Boosting', GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth=10, subsample=0.8)),\n        ('XGBoost', xgb.XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8)),\n        ('LightGBM', lgb.LGBMClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8)),\n    ]),\n'MLP Classifier': MLPClassifier(\n    hidden_layer_sizes=(256, 128, 64),  # Increased hidden layer sizes\n    max_iter=1000,  # Increased maximum number of iterations\n    activation='relu',  # ReLU activation function\n    solver='adam',  # Adam optimizer\n    random_state=42\n)\n}\n\n# Training and evaluating each model\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_cnn, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_scaled, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    else:\n        model.fit(X_train, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T07:37:18.997665Z","iopub.execute_input":"2024-05-05T07:37:18.998233Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-05 07:37:23.577801: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-05 07:37:23.577927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-05 07:37:23.732092: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression: Accuracy = 0.7768, Precision = 0.7377, Recall = 0.7768, F1 Score = 0.7356, Training Time = 0.1730 s, Prediction Time = 0.0327 s\nNaive Bayes: Accuracy = 0.9911, Precision = 0.9913, Recall = 0.9911, F1 Score = 0.9911, Training Time = 0.0976 s, Prediction Time = 0.0449 s\nDecision Tree: Accuracy = 0.9214, Precision = 0.9231, Recall = 0.9214, F1 Score = 0.9220, Training Time = 1.7056 s, Prediction Time = 0.0187 s\nRandom Forest: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 47.6762 s, Prediction Time = 0.1599 s\nKNN: Accuracy = 0.9929, Precision = 0.9929, Recall = 0.9929, F1 Score = 0.9929, Training Time = 0.0303 s, Prediction Time = 0.1770 s\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier0.9911\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport time\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/librosaextractedtess/ExtractedFeaturesForTessDatasetLibrosaCleanedNa.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize LDA\nlda = LDA()\n\n# Fit LDA on the training data and transform both training and test data\nX_train_lda = lda.fit_transform(X_train_scaled, y_train_encoded)\nX_test_lda = lda.transform(X_test_scaled)\n\n# Display the number of components retained after LDA\nprint(f'Number of components retained after LDA: {lda.explained_variance_ratio_.shape[0]}')\n\n# Reshape data for CNN input after LDA\nX_train_lda_cnn = X_train_lda.reshape(X_train_lda.shape[0], X_train_lda.shape[1], 1)\nX_test_lda_cnn = X_test_lda.reshape(X_test_lda.shape[0], X_test_lda.shape[1], 1)\n\n# Initializing models\nmodels = {\n    'Logistic Regression': LogisticRegression(C=1.0, penalty='l2'),\n    'Naive Bayes': GaussianNB(),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5),\n    'KNN': KNeighborsClassifier(n_neighbors=5),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n    'Feedforward Neural Network': Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_lda.shape[1],)),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Convolutional Neural Network': Sequential([\n        Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_lda_cnn.shape[1], X_train_lda_cnn.shape[2])),\n        MaxPooling1D(pool_size=2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Deep Belief Network': Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_lda.shape[1],)),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n        Dense(32, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'SVM': SVC(kernel='linear')\n}\n\n# Initialize base models\nbase_models = [\n    ('Logistic Regression', LogisticRegression(C=1.0, penalty='l2')),\n    ('Naive Bayes', GaussianNB()),\n    ('Decision Tree', DecisionTreeClassifier(max_depth=5)),\n    ('Random Forest', RandomForestClassifier(n_estimators=100, max_depth=5)),\n    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1))\n]\n\n# Initialize ensemble methods\nensemble_methods = {\n    'Voting Classifier': VotingClassifier(estimators=base_models),\n    'Stacking Classifier': StackingClassifier(estimators=base_models, final_estimator=LogisticRegression()),\n    'Bagging Classifier': BaggingClassifier(base_estimator=DecisionTreeClassifier())\n}\n\n# Include ensemble methods in the models dictionary\nmodels.update(ensemble_methods)\n\n# Training and evaluating each model with LDA-transformed data\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_lda, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_lda_cnn, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_lda, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    else:\n        model.fit(X_train_lda, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_lda)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test_lda)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name} with LDA: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T10:03:40.757585Z","iopub.execute_input":"2024-05-05T10:03:40.758034Z","iopub.status.idle":"2024-05-05T10:04:50.229280Z","shell.execute_reply.started":"2024-05-05T10:03:40.757988Z","shell.execute_reply":"2024-05-05T10:04:50.228077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-05 10:03:45.914711: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-05 10:03:45.914839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-05 10:03:46.074393: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Number of components retained after LDA: 6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression with LDA: Accuracy = 0.8054, Precision = 0.8210, Recall = 0.8054, F1 Score = 0.8053, Training Time = 0.0717 s, Prediction Time = 0.0003 s\nNaive Bayes with LDA: Accuracy = 0.8000, Precision = 0.8063, Recall = 0.8000, F1 Score = 0.7943, Training Time = 0.0026 s, Prediction Time = 0.0008 s\nDecision Tree with LDA: Accuracy = 0.7536, Precision = 0.7669, Recall = 0.7536, F1 Score = 0.7488, Training Time = 0.0115 s, Prediction Time = 0.0005 s\nRandom Forest with LDA: Accuracy = 0.7875, Precision = 0.7869, Recall = 0.7875, F1 Score = 0.7821, Training Time = 0.4224 s, Prediction Time = 0.0131 s\nKNN with LDA: Accuracy = 0.8161, Precision = 0.8156, Recall = 0.8161, F1 Score = 0.8113, Training Time = 0.0028 s, Prediction Time = 0.0401 s\nGradient Boosting with LDA: Accuracy = 0.7625, Precision = 0.7641, Recall = 0.7625, F1 Score = 0.7579, Training Time = 4.6005 s, Prediction Time = 0.0096 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nFeedforward Neural Network with LDA: Accuracy = 0.8107, Precision = 0.8160, Recall = 0.8107, F1 Score = 0.8090, Training Time = 3.5891 s, Prediction Time = 0.2008 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nConvolutional Neural Network with LDA: Accuracy = 0.7679, Precision = 0.7806, Recall = 0.7679, F1 Score = 0.7668, Training Time = 3.1075 s, Prediction Time = 0.2060 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nDeep Belief Network with LDA: Accuracy = 0.8071, Precision = 0.8095, Recall = 0.8071, F1 Score = 0.8032, Training Time = 3.8969 s, Prediction Time = 0.1935 s\nSVM with LDA: Accuracy = 0.8179, Precision = 0.8171, Recall = 0.8179, F1 Score = 0.8146, Training Time = 0.0071 s, Prediction Time = 0.0015 s\nVoting Classifier with LDA: Accuracy = 0.8179, Precision = 0.8189, Recall = 0.8179, F1 Score = 0.8142, Training Time = 5.1407 s, Prediction Time = 0.0701 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Stacking Classifier with LDA: Accuracy = 0.8107, Precision = 0.8118, Recall = 0.8107, F1 Score = 0.8071, Training Time = 26.5531 s, Prediction Time = 0.0612 s\nBagging Classifier with LDA: Accuracy = 0.7679, Precision = 0.7749, Recall = 0.7679, F1 Score = 0.7639, Training Time = 0.0957 s, Prediction Time = 0.0026 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nimport time\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions','Path'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Standardize features using StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Reshape data for CNN input\nX_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\nX_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n\n# Initializing models with improved parameters\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, C= 0.001),\n    'Naive Bayes': GaussianNB(var_smoothing=1e-11),\n    'Decision Tree': DecisionTreeClassifier(max_depth= 7, min_samples_split=5),\n    'Random Forest': RandomForestClassifier(n_estimators=1000, max_depth=10, min_samples_split=5),\n    'KNN': KNeighborsClassifier(n_neighbors=15),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth= 10, subsample=0.8),\n    'XGBoost': xgb.XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth= 10, subsample=0.8),\n    'LightGBM': lgb.LGBMClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8),\n    'Feedforward Neural Network': Sequential([\n        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n        Dropout(0.5),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Convolutional Neural Network': Sequential([\n        Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2])),\n        MaxPooling1D(pool_size=2),\n        Flatten(),\n        Dense(256, activation='relu'),\n        Dropout(0.5),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Deep Belief Network': Sequential([\n        Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n        Dropout(0.5),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(64, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Voting Classifier': VotingClassifier(estimators=[\n        ('Logistic Regression', LogisticRegression(max_iter=1000, C= 0.001)),\n        ('Naive Bayes', GaussianNB(var_smoothing=1e-11)),\n        ('Decision Tree', DecisionTreeClassifier(max_depth=7, min_samples_split=5)),\n        ('Random Forest', RandomForestClassifier(n_estimators=1000, max_depth=10, min_samples_split=5)),\n        ('KNN', KNeighborsClassifier(n_neighbors=15)),\n        ('Gradient Boosting', GradientBoostingClassifier(n_estimators=500, learning_rate=0.01, max_depth=10, subsample=0.8)),\n        ('XGBoost', xgb.XGBClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8)),\n        ('LightGBM', lgb.LGBMClassifier(n_estimators=300, learning_rate=0.01, max_depth=10, subsample=0.8)),\n    ]),\n    'MLP Classifier': MLPClassifier(\n        hidden_layer_sizes=(256, 128, 64),\n        max_iter=1000,\n        activation='relu',\n        solver='adam',\n        random_state=42\n    )\n}\n\n# Training and evaluating each model\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        if name == 'Convolutional Neural Network':\n            model.fit(X_train_cnn, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n        else:\n            model.fit(X_train_scaled, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    else:\n        model.fit(X_train, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_scaled)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name}: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T11:04:00.346945Z","iopub.execute_input":"2024-05-31T11:04:00.347474Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-05-31 11:04:06.152071: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-31 11:04:06.152314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-31 11:04:06.305916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression: Accuracy = 0.7768, Precision = 0.7377, Recall = 0.7768, F1 Score = 0.7356, Training Time = 0.2579 s, Prediction Time = 0.0545 s\nNaive Bayes: Accuracy = 0.9911, Precision = 0.9913, Recall = 0.9911, F1 Score = 0.9911, Training Time = 0.1388 s, Prediction Time = 0.0538 s\nDecision Tree: Accuracy = 0.9196, Precision = 0.9241, Recall = 0.9196, F1 Score = 0.9206, Training Time = 1.7385 s, Prediction Time = 0.0254 s\nRandom Forest: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 49.9124 s, Prediction Time = 0.2082 s\nKNN: Accuracy = 0.9929, Precision = 0.9929, Recall = 0.9929, F1 Score = 0.9929, Training Time = 0.0366 s, Prediction Time = 0.2128 s\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nfrom sklearn.decomposition import PCA\nimport time\n\n# Load data from CS\ndata = pd.read_csv('/kaggle/input/librosaextractedtess/ExtractedFeaturesForTessDatasetLibrosaCleanedNa.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize PCA\npca = PCA(n_components=0.95)  # Retain 95% of the variance\n\n# Fit PCA on the training data and transform both training and test data\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Display the number of components retained after PCA\nprint(f'Number of components retained after PCA: {pca.n_components_}')\n\n# Reshape data for CNN input after PCA\nX_train_pca_cnn = X_train_pca.reshape(X_train_pca.shape[0], X_train_pca.shape[1], 1)\nX_test_pca_cnn = X_test_pca.reshape(X_test_pca.shape[0], X_test_pca.shape[1], 1)\n\n# Initializing models\nmodels = {\n    'Logistic Regression': LogisticRegression(C=1.0, penalty='l2'),\n    'Naive Bayes': GaussianNB(),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5),\n    'KNN': KNeighborsClassifier(n_neighbors=5),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n    'Feedforward Neural Network': Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_pca.shape[1],)),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Convolutional Neural Network': Sequential([\n        Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_pca_cnn.shape[1], X_train_pca_cnn.shape[2])),\n        MaxPooling1D(pool_size=2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Deep Belief Network': Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_pca.shape[1],)),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n        Dense(32, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'SVM': SVC(kernel='linear')\n}\n\n# Initialize base models\nbase_models = [\n    ('Logistic Regression', LogisticRegression(C=1.0, penalty='l2')),\n    ('Naive Bayes', GaussianNB()),\n    ('Decision Tree', DecisionTreeClassifier(max_depth=5)),\n    ('Random Forest', RandomForestClassifier(n_estimators=100, max_depth=5)),\n    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1))\n]\n\n# Initialize ensemble methods\nensemble_methods = {\n    'Voting Classifier': VotingClassifier(estimators=base_models),\n    'Stacking Classifier': StackingClassifier(estimators=base_models, final_estimator=LogisticRegression()),\n    'Bagging Classifier': BaggingClassifier(base_estimator=DecisionTreeClassifier())\n}\n\n# Include ensemble methods in the models dictionary\nmodels.update(ensemble_methods)\n\n# Training and evaluating each model with PCA-transformed data\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_pca, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_pca_cnn, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_pca, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    else:\n        model.fit(X_train_pca, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_pca)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test_pca)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name} with PCA: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T10:34:25.186361Z","iopub.execute_input":"2024-05-05T10:34:25.186798Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Number of components retained after PCA: 64\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression with PCA: Accuracy = 0.4518, Precision = 0.4874, Recall = 0.4518, F1 Score = 0.4498, Training Time = 0.2070 s, Prediction Time = 0.0007 s\nNaive Bayes with PCA: Accuracy = 0.4250, Precision = 0.5252, Recall = 0.4250, F1 Score = 0.4351, Training Time = 0.0072 s, Prediction Time = 0.0032 s\nDecision Tree with PCA: Accuracy = 0.4946, Precision = 0.5297, Recall = 0.4946, F1 Score = 0.4885, Training Time = 0.1285 s, Prediction Time = 0.0004 s\nRandom Forest with PCA: Accuracy = 0.5643, Precision = 0.5645, Recall = 0.5643, F1 Score = 0.5396, Training Time = 1.0363 s, Prediction Time = 0.0141 s\nKNN with PCA: Accuracy = 0.4518, Precision = 0.4588, Recall = 0.4518, F1 Score = 0.4486, Training Time = 0.0014 s, Prediction Time = 0.1166 s\nGradient Boosting with PCA: Accuracy = 0.6464, Precision = 0.6432, Recall = 0.6464, F1 Score = 0.6402, Training Time = 36.2285 s, Prediction Time = 0.0098 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nFeedforward Neural Network with PCA: Accuracy = 0.5000, Precision = 0.5039, Recall = 0.5000, F1 Score = 0.4945, Training Time = 3.6634 s, Prediction Time = 0.1917 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\nConvolutional Neural Network with PCA: Accuracy = 0.6036, Precision = 0.6196, Recall = 0.6036, F1 Score = 0.5987, Training Time = 6.8818 s, Prediction Time = 0.2206 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nDeep Belief Network with PCA: Accuracy = 0.3089, Precision = 0.3608, Recall = 0.3089, F1 Score = 0.2893, Training Time = 3.6697 s, Prediction Time = 0.2106 s\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport time\n\n# Load data from CSV\ndata = pd.read_csv('/kaggle/input/wav2vectessfeatures/ExtractedFeaturesForTessDatasetWav2Vec.csv')\n\n# Separate features (X) and labels (y)\nX = data.drop(columns=['Emotions','Path'])\ny = data['Emotions']\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Splitting the dataset into train and test sets\nX_train, X_test, y_train_encoded, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# Scale features using RobustScaler\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize LDA\nlda = LDA()\n\n# Fit LDA on the training data and transform both training and test data\nX_train_lda = lda.fit_transform(X_train_scaled, y_train_encoded)\nX_test_lda = lda.transform(X_test_scaled)\n\n# Display the number of components retained after LDA\nprint(f'Number of components retained after LDA: {lda.explained_variance_ratio_.shape[0]}')\n\n# Reshape data for CNN input after LDA\nX_train_lda_cnn = X_train_lda.reshape(X_train_lda.shape[0], X_train_lda.shape[1], 1)\nX_test_lda_cnn = X_test_lda.reshape(X_test_lda.shape[0], X_test_lda.shape[1], 1)\n\n# Initializing models\nmodels = {\n    'Logistic Regression': LogisticRegression(C=1.0, penalty='l2'),\n    'Naive Bayes': GaussianNB(),\n    'Decision Tree': DecisionTreeClassifier(max_depth=5),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5),\n    'KNN': KNeighborsClassifier(n_neighbors=5),\n    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n    'Feedforward Neural Network': Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_lda.shape[1],)),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Convolutional Neural Network': Sequential([\n        Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_lda_cnn.shape[1], X_train_lda_cnn.shape[2])),\n        MaxPooling1D(pool_size=2),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'Deep Belief Network': Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_lda.shape[1],)),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n        Dense(32, activation='relu'),\n        Dense(len(label_encoder.classes_), activation='softmax')\n    ]),\n    'SVM': SVC(kernel='linear')\n}\n\n# Initialize base models\nbase_models = [\n    ('Logistic Regression', LogisticRegression(C=1.0, penalty='l2')),\n    ('Naive Bayes', GaussianNB()),\n    ('Decision Tree', DecisionTreeClassifier(max_depth=5)),\n    ('Random Forest', RandomForestClassifier(n_estimators=100, max_depth=5)),\n    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1))\n]\n\n# Initialize ensemble methods\nensemble_methods = {\n    'Voting Classifier': VotingClassifier(estimators=base_models),\n    'Stacking Classifier': StackingClassifier(estimators=base_models, final_estimator=LogisticRegression()),\n    'Bagging Classifier': BaggingClassifier(base_estimator=DecisionTreeClassifier())\n}\n\n# Include ensemble methods in the models dictionary\nmodels.update(ensemble_methods)\n\n# Training and evaluating each model with LDA-transformed data\nfor name, model in models.items():\n    start_time = time.time()\n    \n    if name == 'Feedforward Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_lda, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Convolutional Neural Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_lda_cnn, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    elif name == 'Deep Belief Network':\n        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train_lda, y_train_encoded, epochs=20, batch_size=64, verbose=0)\n    else:\n        model.fit(X_train_lda, y_train_encoded)\n    \n    train_time = time.time() - start_time\n    \n    start_time = time.time()\n    if name in ['Feedforward Neural Network', 'Convolutional Neural Network', 'Deep Belief Network']:\n        y_pred_proba = model.predict(X_test_lda)\n        y_pred = y_pred_proba.argmax(axis=-1)\n    else:\n        y_pred = model.predict(X_test_lda)\n    \n    predict_time = time.time() - start_time\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    \n    print(f'{name} with LDA: Accuracy = {accuracy:.4f}, Precision = {precision:.4f}, Recall = {recall:.4f}, F1 Score = {f1:.4f}, Training Time = {train_time:.4f} s, Prediction Time = {predict_time:.4f} s')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T10:15:37.488474Z","iopub.execute_input":"2024-05-05T10:15:37.488971Z","iopub.status.idle":"2024-05-05T10:16:26.926100Z","shell.execute_reply.started":"2024-05-05T10:15:37.488931Z","shell.execute_reply":"2024-05-05T10:16:26.924811Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Number of components retained after LDA: 6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.0452 s, Prediction Time = 0.0003 s\nNaive Bayes with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.0023 s, Prediction Time = 0.0008 s\nDecision Tree with LDA: Accuracy = 0.8268, Precision = 0.8850, Recall = 0.8268, F1 Score = 0.7913, Training Time = 0.0076 s, Prediction Time = 0.0003 s\nRandom Forest with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.4118 s, Prediction Time = 0.0133 s\nKNN with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.0024 s, Prediction Time = 0.0380 s\nGradient Boosting with LDA: Accuracy = 0.9714, Precision = 0.9733, Recall = 0.9714, F1 Score = 0.9718, Training Time = 4.1912 s, Prediction Time = 0.0065 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nFeedforward Neural Network with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 4.0592 s, Prediction Time = 0.2124 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\nConvolutional Neural Network with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 3.5504 s, Prediction Time = 0.2137 s\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\nDeep Belief Network with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 3.6789 s, Prediction Time = 0.2109 s\nSVM with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 0.0049 s, Prediction Time = 0.0011 s\nVoting Classifier with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 4.8048 s, Prediction Time = 0.0667 s\nStacking Classifier with LDA: Accuracy = 1.0000, Precision = 1.0000, Recall = 1.0000, F1 Score = 1.0000, Training Time = 24.3731 s, Prediction Time = 0.0538 s\nBagging Classifier with LDA: Accuracy = 0.9875, Precision = 0.9880, Recall = 0.9875, F1 Score = 0.9875, Training Time = 0.1078 s, Prediction Time = 0.0026 s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"}]}]}